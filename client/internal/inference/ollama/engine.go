package ollama

import (
	"context"
	"encoding/base64"
	"encoding/json"
	"fmt"
	"log"
	"net/http"
	"net/url"
	"star-fire/client/internal/config"
	"star-fire/pkg/public"
	"strings"
	"time"

	"github.com/gorilla/websocket"
	"github.com/ollama/ollama/api"
	"github.com/sashabaranov/go-openai"
)

type Engine struct {
	client    *api.Client
	models    map[string]api.ProcessModelResponse
	ollamaURL string

	thinkingStarted map[string]bool // fingerprint -> æ˜¯å¦å·²ç»å¼€å§‹æ€è€ƒ
}

func NewEngine(ctx context.Context, ollamaURL string, conf *config.Config) (*Engine, error) {
	engine := &Engine{
		models:          make(map[string]api.ProcessModelResponse),
		ollamaURL:       ollamaURL,
		thinkingStarted: make(map[string]bool),
	}

	if err := engine.Initialize(ctx, conf); err != nil {
		return nil, err
	}
	return engine, nil
}

func (e *Engine) Name() string {
	return "ollama"
}

func (e *Engine) Initialize(ctx context.Context, conf *config.Config) error {
	httpClient := &http.Client{
		Timeout: 3600 * time.Second,
	}

	ollamaURL, err := url.Parse(e.ollamaURL)
	if err != nil {
		return fmt.Errorf("invalid Ollama URL: %w", err)
	}
	e.client = api.NewClient(ollamaURL, httpClient)

	timeoutCtx, cancel := context.WithTimeout(ctx, 5*time.Second)
	defer cancel()

	if err := e.client.Heartbeat(timeoutCtx); err != nil {
		return fmt.Errorf("can not connect to ollama engine: %w", err)
	}
	if _, err := e.ListModels(ctx, conf); err != nil {
		log.Printf("alert: load models error: %v", err)
	}

	return nil
}

func (e *Engine) ListModels(ctx context.Context, conf *config.Config) ([]*public.Model, error) {
	var allModels []*public.Model

	// 1. é¦–å…ˆè·å–æ‰€æœ‰å·²ä¸‹è½½çš„æ¨¡å‹ï¼ˆç”¨äºå‘ç°embeddingæ¨¡å‹ï¼‰
	allDownloadedResp, err := e.client.List(ctx)
	if err != nil {
		return nil, fmt.Errorf("get ollama downloaded models error: %w", err)
	}

	// 2. è·å–æ­£åœ¨è¿è¡Œçš„æ¨¡å‹ï¼ˆç”¨äºæ™®é€šæ¨¡å‹ï¼‰
	runningResp, err := e.client.ListRunning(ctx)
	if err != nil {
		log.Printf("get ollama running models error: %v", err)
		// å¦‚æœè·å–è¿è¡Œä¸­çš„æ¨¡å‹å¤±è´¥ï¼Œç»§ç»­å¤„ç†å·²ä¸‹è½½çš„æ¨¡å‹
	}

	// åˆ›å»ºè¿è¡Œä¸­æ¨¡å‹çš„æ˜ å°„ï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾
	runningModels := make(map[string]api.ProcessModelResponse)
	if runningResp != nil {
		for _, model := range runningResp.Models {
			runningModels[model.Name] = model
			// å°†è¿è¡Œä¸­çš„æ¨¡å‹ä¿¡æ¯å­˜å‚¨åˆ°å†…éƒ¨mapä¸­
			e.models[model.Name] = model
		}
	}

	// 3. å¤„ç†æ‰€æœ‰å·²ä¸‹è½½çš„æ¨¡å‹
	for _, model := range allDownloadedResp.Models {
		isEmbedding := isOllamaEmbeddingModel(model.Name)
		isRunning := false

		// æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ­£åœ¨è¿è¡Œ
		if runningModel, exists := runningModels[model.Name]; exists {
			isRunning = true
			// ä½¿ç”¨è¿è¡Œä¸­æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯
			processModel := runningModel
			e.models[model.Name] = processModel
		} else {
			// å¯¹äºéè¿è¡Œä¸­çš„æ¨¡å‹ï¼Œåˆ›å»ºåŸºæœ¬ä¿¡æ¯
			processModel := api.ProcessModelResponse{
				Name:   model.Name,
				Size:   model.Size,
				Digest: model.Digest,
			}
			e.models[model.Name] = processModel
		}

		// 4. å†³å®šæ˜¯å¦æ³¨å†Œæ¨¡å‹
		shouldRegister := false
		modelType := "ollama"

		if isEmbedding {
			// Embeddingæ¨¡å‹ï¼šæ— è®ºæ˜¯å¦è¿è¡Œéƒ½æ³¨å†Œ
			shouldRegister = true
			modelType = "embedding"
			log.Printf("Found Ollama embedding model: %s (running: %v)", model.Name, isRunning)
		} else if isRunning {
			// æ™®é€šæ¨¡å‹ï¼šåªæ³¨å†Œè¿è¡Œä¸­çš„
			shouldRegister = true
			log.Printf("Found running Ollama model: %s", model.Name)
		} else {
			// æ™®é€šæ¨¡å‹ä¸”æœªè¿è¡Œï¼šä¸æ³¨å†Œ
			log.Printf("Skipping non-running model: %s", model.Name)
		}

		// 5. å¦‚æœå†³å®šæ³¨å†Œï¼Œæ·»åŠ åˆ°ç»“æœåˆ—è¡¨
		if shouldRegister {
			publicModel := &public.Model{
				Name: model.Name,
				Type: modelType,
				Size: fmt.Sprintf("%d", model.Size),
				Arch: model.Details.QuantizationLevel,
				IPPM: conf.InputTokenPricePerMillion,  // æ¯ç™¾ä¸‡è¾“å…¥tokensä»·æ ¼
				OPPM: conf.OutputTokenPricePerMillion, // æ¯ç™¾ä¸‡è¾“å‡ºtokensä»·æ ¼
			}
			allModels = append(allModels, publicModel)
		}
	}

	log.Printf("Ollama registered %d models (%d embedding models)",
		len(allModels), countEmbeddingModels(allModels))
	return allModels, nil
}

// countEmbeddingModels ç»Ÿè®¡embeddingæ¨¡å‹æ•°é‡
func countEmbeddingModels(models []*public.Model) int {
	count := 0
	for _, model := range models {
		if model.Type == "embedding" {
			count++
		}
	}
	return count
}

// isOllamaEmbeddingModel æ£€æŸ¥Ollamaæ¨¡å‹æ˜¯å¦ä¸ºembeddingæ¨¡å‹
func isOllamaEmbeddingModel(modelName string) bool {
	// Ollamaä¸­å¸¸è§çš„embeddingæ¨¡å‹
	embeddingModels := []string{
		// BGEæ¨¡å‹ç³»åˆ—
		"bge-large",
		"bge-base",
		"bge-small",
		"bge-large-en",
		"bge-base-en",
		"bge-small-en",
		"bge-large-zh",
		"bge-base-zh",
		"bge-small-zh",
		"bge-m3",
		"bge-reranker",

		// å…¶ä»–embeddingæ¨¡å‹
		"all-minilm",
		"all-mpnet",
		"nomic-embed",
		"snowflake-arctic-embed",
		"mxbai-embed",
		"paraphrase-multilingual",
		"sentence-transformers",

		// å¤šè¯­è¨€embeddingæ¨¡å‹
		"multilingual-e5",
		"e5-large",
		"e5-base",
		"e5-small",
	}

	modelLower := strings.ToLower(modelName)

	// ç²¾ç¡®åŒ¹é…æˆ–åŒ…å«åŒ¹é…
	for _, embeddingModel := range embeddingModels {
		if strings.Contains(modelLower, embeddingModel) {
			return true
		}
	}

	// å…³é”®è¯åŒ¹é…
	embeddingKeywords := []string{
		"embed", "embedding", "embeddings",
		"sentence", "semantic", "similarity",
		"retrieval", "rerank", "reranker",
		"vector", "encode", "encoder",
	}

	for _, keyword := range embeddingKeywords {
		if strings.Contains(modelLower, keyword) {
			return true
		}
	}

	return false
}

// æ·»åŠ è½¬æ¢å‡½æ•°
func convertOpenAIToolToOllama(openaiTool openai.Tool) (api.Tool, error) {
	ollamaTool := api.Tool{
		Type: "function", // OpenAI çš„ ToolType æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œç›´æ¥ä½¿ç”¨ "function"
		Function: api.ToolFunction{
			Name:        openaiTool.Function.Name,
			Description: openaiTool.Function.Description,
		},
	}

	// è½¬æ¢ Parameters
	if openaiTool.Function.Parameters != nil {
		// å°† OpenAI çš„ Parameters è½¬æ¢ä¸º JSON å†è§£æåˆ° Ollama æ ¼å¼
		paramBytes, err := json.Marshal(openaiTool.Function.Parameters)
		if err != nil {
			return api.Tool{}, fmt.Errorf("marshal parameters error: %v", err)
		}

		var paramMap map[string]interface{}
		if err := json.Unmarshal(paramBytes, &paramMap); err != nil {
			return api.Tool{}, fmt.Errorf("unmarshal parameters error: %v", err)
		}

		// è®¾ç½®åŸºæœ¬ç±»å‹
		if typeVal, ok := paramMap["type"].(string); ok {
			ollamaTool.Function.Parameters.Type = typeVal
		} else {
			ollamaTool.Function.Parameters.Type = "object" // é»˜è®¤å€¼
		}

		// è®¾ç½® Required å­—æ®µ
		if requiredVal, ok := paramMap["required"].([]interface{}); ok {
			required := make([]string, len(requiredVal))
			for i, req := range requiredVal {
				if reqStr, ok := req.(string); ok {
					required[i] = reqStr
				}
			}
			ollamaTool.Function.Parameters.Required = required
		}

		// è®¾ç½® Properties å­—æ®µ
		if propertiesVal, ok := paramMap["properties"].(map[string]interface{}); ok {
			properties := make(map[string]struct {
				Type        api.PropertyType `json:"type"`
				Items       any              `json:"items,omitempty"`
				Description string           `json:"description"`
				Enum        []any            `json:"enum,omitempty"`
			})

			for propName, propVal := range propertiesVal {
				if propMap, ok := propVal.(map[string]interface{}); ok {
					property := struct {
						Type        api.PropertyType `json:"type"`
						Items       any              `json:"items,omitempty"`
						Description string           `json:"description"`
						Enum        []any            `json:"enum,omitempty"`
					}{}

					// è®¾ç½®ç±»å‹
					if typeVal, ok := propMap["type"]; ok {
						if typeStr, ok := typeVal.(string); ok {
							property.Type = api.PropertyType([]string{typeStr})
						} else if typeSlice, ok := typeVal.([]interface{}); ok {
							types := make([]string, len(typeSlice))
							for i, t := range typeSlice {
								if tStr, ok := t.(string); ok {
									types[i] = tStr
								}
							}
							property.Type = api.PropertyType(types)
						}
					}

					// è®¾ç½®æè¿°
					if descVal, ok := propMap["description"].(string); ok {
						property.Description = descVal
					}

					// è®¾ç½®æšä¸¾
					if enumVal, ok := propMap["enum"].([]interface{}); ok {
						property.Enum = enumVal
					}

					// è®¾ç½® Items
					if itemsVal, ok := propMap["items"]; ok {
						property.Items = itemsVal
					}

					properties[propName] = property
				}
			}

			// ç±»å‹è½¬æ¢ï¼šå°† map[string]struct{...} è½¬æ¢ä¸º map[string]api.ToolProperty
			toolProperties := make(map[string]api.ToolProperty)
			for key, value := range properties {
				toolProperties[key] = api.ToolProperty{
					Type:        value.Type,
					Items:       value.Items,
					Description: value.Description,
					Enum:        value.Enum,
				}
			}
			ollamaTool.Function.Parameters.Properties = toolProperties
		}

		// è®¾ç½®å…¶ä»–å­—æ®µ
		if defsVal, ok := paramMap["$defs"]; ok {
			ollamaTool.Function.Parameters.Defs = defsVal
		}

		if itemsVal, ok := paramMap["items"]; ok {
			ollamaTool.Function.Parameters.Items = itemsVal
		}
	}

	return ollamaTool, nil
}

func (e *Engine) HandleChat(ctx context.Context, fingerprint string,
	request *openai.ChatCompletionRequest, responseConn *websocket.Conn) error {
	// think å¼€å…³
	think := &api.ThinkValue{}
	if strings.Index(request.Model, "qwen") >= 0 || strings.Index(request.Model, "DeepSeek v3.1") >= 0 || strings.Index(request.Model, "DeepSeek v3.2-exp") >= 0 {
		// qwenç³»åˆ— æ”¯æŒthink å¼€å…³çš„ç”¨ enable_thinking (true | false) ï¼Œéœ€è¦å°†openai API reasoning_effort
		if request.ReasoningEffort == "none" || request.ReasoningEffort == "" {
			think.Value = false
		} else {
			think.Value = true
		}
	}
	if strings.Contains(request.Model, "Doubao-seed-1-6") || strings.Contains(request.Model, "gpt-5.1") {
		think.Value = request.ReasoningEffort
	}
	ollamaReq := &api.ChatRequest{
		Model:    request.Model,
		Stream:   &request.Stream,
		Messages: convertToOllamaMessages(request.Messages),
		Options: map[string]interface{}{
			"temperature":      request.Temperature,
			"top_p":            request.TopP,
			"max_tokens":       request.MaxTokens,
			"reasoning_effort": request.ReasoningEffort,
			"stream_options":   map[string]interface{}{},
			//"num_predict":      -1,
		},
		Think: think,
		Tools: make([]api.Tool, 0, len(request.Tools)),
	}
	for _, tool := range request.Tools {
		if &tool == nil {
			continue
		}
		ollamaTool, err := convertOpenAIToolToOllama(tool)
		if err != nil {
			log.Printf("convert tool error: %v", err)
			continue
		}
		ollamaReq.Tools = append(ollamaReq.Tools, ollamaTool)
	}

	respFunc := func(resp api.ChatResponse) error {
		//fmt.Println("ollama request is:", *ollamaReq)
		if request.Stream {
			openAIStreamResp := e.convertToOpenAIStreamResponse(&resp, fingerprint)
			//fmt.Println("stream resp message.role is:", resp.Message.Role, "c=", resp.Message.Content, "len(content)=", len(resp.Message.Content))
			err := responseConn.WriteJSON(public.WSMessage{
				Type:        public.MESSAGE_STREAM,
				Content:     openAIStreamResp,
				FingerPrint: fingerprint,
			})
			if err != nil {
				log.Printf("send message with websocket error: %v", err)
				err = responseConn.WriteJSON(public.WSMessage{
					Type:        public.CLOSE,
					Content:     nil,
					FingerPrint: fingerprint,
				})
				return err
			}
		} else if resp.Done {
			openAIResp := convertToOpenAIResponse(&resp, fingerprint)
			err := responseConn.WriteJSON(public.WSMessage{
				Type:        public.MESSAGE,
				Content:     openAIResp,
				FingerPrint: fingerprint,
			})
			if err != nil {
				log.Printf("send message with websocket error: %v", err)
				return err
			}
			return responseConn.WriteJSON(public.WSMessage{
				Type:        public.CLOSE,
				Content:     nil,
				FingerPrint: fingerprint,
			})
		}

		return nil
	}

	err := e.client.Chat(ctx, ollamaReq, respFunc)
	if err != nil {
		log.Printf("Ollama chat error: %v", err)
		err = responseConn.WriteJSON(public.WSMessage{
			Type:        public.MODEL_ERROR,
			Content:     err.Error(),
			FingerPrint: fingerprint,
		})
		return err
	}

	return nil
}

func convertToOllamaMessages(messages []openai.ChatCompletionMessage) []api.Message {
	ollamaMessages := make([]api.Message, len(messages))
	index := 0
	for i, msg := range messages {
		ollamaMessage := api.Message{
			Role:    msg.Role,
			Content: msg.Content,
		}

		// å¤„ç†å¤šåª’ä½“å†…å®¹ï¼ˆåŒ…æ‹¬å›¾ç‰‡ï¼‰
		if len(msg.MultiContent) > 0 {
			var images []api.ImageData
			var textContent string

			for _, part := range msg.MultiContent {
				if part.Type == openai.ChatMessagePartTypeText {
					if textContent != "" {
						textContent += "\n"
					}
					textContent += part.Text
				} else if part.Type == openai.ChatMessagePartTypeImageURL && part.ImageURL != nil {
					imageURL := part.ImageURL.URL
					if strings.HasPrefix(imageURL, "data:image/") {
						// æå– base64 éƒ¨åˆ†
						if commaIndex := strings.Index(imageURL, ","); commaIndex != -1 {
							base64Data := imageURL[commaIndex+1:]
							// è§£ç  base64 ä¸ºäºŒè¿›åˆ¶æ•°æ®
							if decodedData, err := base64.StdEncoding.DecodeString(base64Data); err == nil {
								images = append(images, decodedData)
							}
						}
					}
				}
			}

			ollamaMessage.Content = textContent
			ollamaMessage.Images = images
		}

		//fmt.Println("function call:", msg.FunctionCall)
		//fmt.Println("tool calls:", msg.ToolCalls)
		if msg.FunctionCall != nil {
			ollamaMessage.ToolCalls = make([]api.ToolCall, 0, 1)
			var args api.ToolCallFunctionArguments
			if err := json.Unmarshal([]byte(msg.FunctionCall.Arguments), &args); err != nil {
				log.Printf("failed to unmarshal function call arguments: %v", err)
				args = make(api.ToolCallFunctionArguments)
			}
			ollamaMessage.ToolCalls = append(ollamaMessage.ToolCalls, api.ToolCall{
				Function: api.ToolCallFunction{
					Index:     index,
					Name:      msg.FunctionCall.Name,
					Arguments: args,
				},
			})
			index++
		}

		ollamaMessages[i] = ollamaMessage
	}
	return ollamaMessages
}

func (e *Engine) convertToOpenAIStreamResponse(resp *api.ChatResponse, fingerprint string) openai.ChatCompletionStreamResponse {
	var finishReason openai.FinishReason
	if resp.Done {
		if len(resp.Message.ToolCalls) > 0 {
			finishReason = openai.FinishReasonToolCalls
		} else {
			finishReason = openai.FinishReasonStop
		}
	}
	content := ""
	// ğŸ”¥ å…³é”®ï¼šåªåœ¨ç¬¬ä¸€æ¬¡æœ‰ thinking æ—¶æ·»åŠ å¼€å§‹æ ‡ç­¾
	if resp.Message.Thinking != "" {
		if !e.thinkingStarted[fingerprint] {
			content = "<think>\n"
			e.thinkingStarted[fingerprint] = true
		}
		content += resp.Message.Thinking
	}

	// å¦‚æœ thinking ç»“æŸï¼Œcontent å¼€å§‹
	if resp.Message.Content != "" && e.thinkingStarted[fingerprint] && resp.Message.Thinking == "" {
		// thinking åˆšç»“æŸï¼Œæ·»åŠ ç»“æŸæ ‡ç­¾
		content = "\n</think>\n\n" + resp.Message.Content
		e.thinkingStarted[fingerprint] = false // æ ‡è®° thinking å·²ç»“æŸ
	} else if resp.Message.Content != "" {
		content += resp.Message.Content
	}

	realResp := openai.ChatCompletionStreamResponse{
		ID:      fingerprint,
		Object:  "chat.completion.chunk",
		Created: time.Now().Unix(),
		Model:   resp.Model,
		Choices: []openai.ChatCompletionStreamChoice{
			{
				Index: 0,
				Delta: openai.ChatCompletionStreamChoiceDelta{
					Content: content,
					Role: func() string {
						if !resp.Done {
							return "assistant"
						}
						return ""
					}(),
					ToolCalls: func() []openai.ToolCall {
						if len(resp.Message.ToolCalls) > 0 {
							toolCalls := make([]openai.ToolCall, len(resp.Message.ToolCalls))
							for i, tc := range resp.Message.ToolCalls {
								toolCalls[i] = openai.ToolCall{
									Index: &i,
									ID:    fmt.Sprintf("toolcall-%d", i),
									Type:  "function",
									Function: openai.FunctionCall{
										Name: tc.Function.Name,
										Arguments: func() string {
											argsBytes, err := json.Marshal(tc.Function.Arguments)
											if err != nil {
												return ""
											}
											return string(argsBytes)
										}(),
									},
								}
							}
							return toolCalls
						}
						return nil
					}(),
				},
				FinishReason: finishReason,
			},
		},
	}
	if resp.Done {
		realResp.Usage = &openai.Usage{
			PromptTokens:     resp.PromptEvalCount,
			CompletionTokens: resp.EvalCount,
			TotalTokens:      resp.PromptEvalCount + resp.EvalCount,
		}
	}
	return realResp
}

func convertToOpenAIResponse(resp *api.ChatResponse, fingerprint string) openai.ChatCompletionResponse {
	realResp := openai.ChatCompletionResponse{
		ID:      fingerprint,
		Object:  "chat.completion",
		Created: time.Now().Unix(),
		Model:   resp.Model,
		Choices: []openai.ChatCompletionChoice{
			{
				Index: 0,
				Message: openai.ChatCompletionMessage{
					Role:    resp.Message.Role,
					Content: resp.Message.Content,
					FunctionCall: func() *openai.FunctionCall {
						fmt.Println("function call:", resp.Message.ToolCalls)
						if len(resp.Message.ToolCalls) > 0 {
							tc := resp.Message.ToolCalls[0]
							return &openai.FunctionCall{
								Name: tc.Function.Name,
								Arguments: func() string {
									argsBytes, err := json.Marshal(tc.Function.Arguments)
									if err != nil {
										return ""
									}
									return string(argsBytes)
								}(),
							}
						}
						return nil
					}(),
					ToolCalls: func() []openai.ToolCall {
						fmt.Println("tool calls:", resp.Message.ToolCalls)
						if len(resp.Message.ToolCalls) > 0 {
							toolCalls := make([]openai.ToolCall, len(resp.Message.ToolCalls))
							for i, tc := range resp.Message.ToolCalls {
								toolCalls[i] = openai.ToolCall{
									Index: &i,
									ID:    fmt.Sprintf("toolcall-%d", i),
									Type:  "function",
									Function: openai.FunctionCall{
										Name: tc.Function.Name,
										Arguments: func() string {
											argsBytes, err := json.Marshal(tc.Function.Arguments)
											if err != nil {
												return ""
											}
											return string(argsBytes)
										}(),
									},
								}
							}
							return toolCalls
						}
						return nil
					}(),
				},
				FinishReason: func() openai.FinishReason {
					if len(resp.Message.ToolCalls) > 0 {
						return openai.FinishReasonToolCalls
					}
					return openai.FinishReasonStop
				}(),
			},
		},
	}
	if resp.Done {
		realResp.Usage = openai.Usage{
			PromptTokens:     resp.PromptEvalCount,
			CompletionTokens: resp.EvalCount,
			TotalTokens:      resp.PromptEvalCount + resp.EvalCount,
		}
	}
	return realResp
}

func (e *Engine) HandleEmbedding(ctx context.Context, fingerprint string,
	request *openai.EmbeddingRequest, responseConn *websocket.Conn) error {
	log.Printf("handle embedding request [%s]: model=%s, input=%v", fingerprint, request.Model, request.Input)

	// ä½¿ç”¨Ollamaçš„embedding API
	var inputTexts []string

	// å¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥
	switch input := request.Input.(type) {
	case string:
		inputTexts = []string{input}
	case []string:
		inputTexts = input
	case []interface{}:
		for _, item := range input {
			if str, ok := item.(string); ok {
				inputTexts = append(inputTexts, str)
			}
		}
	default:
		errMsg := "unsupported input type for embedding"
		log.Printf("[%s] %s", fingerprint, errMsg)
		return responseConn.WriteJSON(public.WSMessage{
			Type:        public.MODEL_ERROR,
			Content:     errMsg,
			FingerPrint: fingerprint,
		})
	}

	// ä¸ºæ¯ä¸ªè¾“å…¥æ–‡æœ¬ç”Ÿæˆembedding
	var embeddings []openai.Embedding
	for i, text := range inputTexts {
		// ä½¿ç”¨Ollamaçš„embed API
		embedReq := &api.EmbedRequest{
			Model: string(request.Model),
			Input: text,
		}

		embedResp, err := e.client.Embed(ctx, embedReq)
		if err != nil {
			errMsg := fmt.Sprintf("create embedding error: %v", err)
			log.Printf("[%s] %s", fingerprint, errMsg)
			return responseConn.WriteJSON(public.WSMessage{
				Type:        public.MODEL_ERROR,
				Content:     errMsg,
				FingerPrint: fingerprint,
			})
		}

		embedding := openai.Embedding{
			Object:    "embedding",
			Index:     i,
			Embedding: embedResp.Embeddings[0], // Ollamaè¿”å›ï¿½ï¿½ï¿½embeddingå‘é‡
		}
		embeddings = append(embeddings, embedding)
	}

	// æ„é€ OpenAIæ ¼å¼çš„å“åº”
	response := openai.EmbeddingResponse{
		Object: "list",
		Model:  openai.EmbeddingModel(request.Model),
		Data:   embeddings,
		Usage: openai.Usage{
			PromptTokens: calculateInputTokens(inputTexts),
			TotalTokens:  calculateInputTokens(inputTexts),
		},
	}

	log.Printf("[%s] receive embedding response", fingerprint)
	err := responseConn.WriteJSON(public.WSMessage{
		Type:        public.EMBEDDING_RESPONSE,
		Content:     response,
		FingerPrint: fingerprint,
	})

	if err != nil {
		log.Printf("[%s] send embedding response error: %v", fingerprint, err)
		return err
	}

	log.Printf("[%s] send close message", fingerprint)
	err = responseConn.WriteJSON(public.WSMessage{
		Type:        public.CLOSE,
		Content:     nil,
		FingerPrint: fingerprint,
	})
	log.Printf("[%s] send close message success", fingerprint)
	return err
}

func (e *Engine) SupportsEmbedding(modelName string) bool {
	// æ£€æŸ¥æ˜¯å¦ä¸ºembeddingæ¨¡å‹ä¸”åœ¨å¯ç”¨æ¨¡å‹åˆ—è¡¨ä¸­
	if !isOllamaEmbeddingModel(modelName) {
		return false
	}

	// æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
	_, exists := e.models[modelName]
	return exists
}

func (e *Engine) SupportsModel(modelName string, conf *config.Config) bool {
	// æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨å·²ä¸‹è½½çš„æ¨¡å‹åˆ—è¡¨ä¸­
	if _, exists := e.models[modelName]; exists {
		return true
	}

	// å¦‚æœæ¨¡å‹ä¸åœ¨ç¼“å­˜ä¸­ï¼Œå°è¯•é‡æ–°è·å–æ¨¡å‹åˆ—è¡¨
	ctx, cancel := context.WithTimeout(context.Background(), 3*time.Second)
	defer cancel()

	// æ›´æ–°æ¨¡å‹åˆ—è¡¨
	if _, err := e.ListModels(ctx, conf); err != nil {
		log.Printf("failed to refresh models: %v", err)
		return false
	}

	// å†æ¬¡æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
	_, exists := e.models[modelName]
	return exists
}

// calculateInputTokens ä¼°ç®—è¾“å…¥æ–‡æœ¬çš„tokenæ•°é‡
func calculateInputTokens(texts []string) int {
	totalTokens := 0
	for _, text := range texts {
		// ç®€å•ä¼°ç®—ï¼šå¹³å‡æ¯4ä¸ªå­—ç¬¦çº¦ç­‰äº1ä¸ªtoken
		tokens := len(text) / 4
		if tokens < 1 {
			tokens = 1
		}
		totalTokens += tokens
	}
	return totalTokens
}
